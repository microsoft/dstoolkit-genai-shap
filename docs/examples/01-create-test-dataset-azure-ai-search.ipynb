{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ContextPrecision,\n",
    "    ContextRecall\n",
    ")\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from ragas.embeddings import LlamaIndexEmbeddingsWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from common import create_search_index,create_search_datasource,create_search_skillset, create_search_indexer\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites:\n",
    "- Azure Subscription\n",
    "- Azure AI Search Service\n",
    "- Azure Storage Account with a container\n",
    "- Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download sample dataset and upload it to the container in blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "Loading files: 100%|██████████| 1/1 [00:00<00:00,  1.39file/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>In the essay, the author discusses the develop...</td>\n",
       "      <td>[It was missing a lot of things you'd want in ...</td>\n",
       "      <td>The author developed a new programming languag...</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Paul Graham mentions a strategy he used for wr...</td>\n",
       "      <td>[It's not that unprestigious types of work are...</td>\n",
       "      <td>Paul Graham found that giving talks was an eff...</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Paul Graham mentions his experience of leaving...</td>\n",
       "      <td>[[18] The worst thing about leaving YC was not...</td>\n",
       "      <td>Paul Graham describes his experience of leavin...</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Describe the pivotal moment in the essay when ...</td>\n",
       "      <td>[Meanwhile I'd been hearing more and more abou...</td>\n",
       "      <td>The pivotal moment in the essay when the autho...</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the essay, the author discusses his initial...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "      <td>The two main influences that initially drew th...</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "      <td>ai (gpt-4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "36  In the essay, the author discusses the develop...   \n",
       "27  Paul Graham mentions a strategy he used for wr...   \n",
       "43  Paul Graham mentions his experience of leaving...   \n",
       "15  Describe the pivotal moment in the essay when ...   \n",
       "2   In the essay, the author discusses his initial...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "36  [It was missing a lot of things you'd want in ...   \n",
       "27  [It's not that unprestigious types of work are...   \n",
       "43  [[18] The worst thing about leaving YC was not...   \n",
       "15  [Meanwhile I'd been hearing more and more abou...   \n",
       "2   [I couldn't have put this into words when I wa...   \n",
       "\n",
       "                                     reference_answer reference_answer_by  \\\n",
       "36  The author developed a new programming languag...          ai (gpt-4)   \n",
       "27  Paul Graham found that giving talks was an eff...          ai (gpt-4)   \n",
       "43  Paul Graham describes his experience of leavin...          ai (gpt-4)   \n",
       "15  The pivotal moment in the essay when the autho...          ai (gpt-4)   \n",
       "2   The two main influences that initially drew th...          ai (gpt-4)   \n",
       "\n",
       "      query_by  \n",
       "36  ai (gpt-4)  \n",
       "27  ai (gpt-4)  \n",
       "43  ai (gpt-4)  \n",
       "15  ai (gpt-4)  \n",
       "2   ai (gpt-4)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset, documents = download_llama_dataset(\n",
    "    llama_dataset_class=\"PaulGrahamEssayDataset\",\n",
    "    download_dir=\"./data\",\n",
    "    show_progress=True\n",
    ")\n",
    "texts = [doc.text_resource.text for doc in documents]\n",
    "df_ragas = rag_dataset.to_pandas()\n",
    "df_ragas.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.text_resource.text for doc in documents]\n",
    "file_path = \"PaulGrahamEssayDataset.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure Blob Storage Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_container = os.environ[\"AZURE_BLOB_CONTAINER\"]\n",
    "blob_connection_string = os.environ[\"AZURE_BLOB_CONNECTION_STRING\"]\n",
    "blob_account_url = os.environ[\"AZURE_BLOB_ACCOUNT_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload sample data to blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "def open_blob_client():\n",
    "    if not blob_connection_string.startswith(\"ResourceId\"):\n",
    "        return BlobServiceClient.from_connection_string(\n",
    "            blob_connection_string,\n",
    "            max_block_size=1024*1024*8, # 8 MiB\n",
    "            max_single_put_size=1024*1024*8 # 8 MiB\n",
    "        )\n",
    "    return BlobServiceClient(\n",
    "        account_url=blob_account_url,\n",
    "        credential=DefaultAzureCredential(),\n",
    "        max_block_size=1024*1024*8, # 8 MiB\n",
    "        max_single_put_size=1024*1024*8 # 8 MiB\n",
    "    )\n",
    "\n",
    "blob_client = open_blob_client()\n",
    "container_client = blob_client.get_container_client(blob_container)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "blob_name = os.path.basename(file_path)\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "if not blob_client.exists():\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        blob_client.upload_blob(data=f, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure OpenAI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "azure_openai_embedding_deployment_id = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure AI Search Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_endpoint  = os.getenv('AZURE_SEARCH_SERVICE_ENDPOINT')\n",
    "api_key = os.getenv('AZURE_SEARCH_API_KEY')\n",
    "search_index  = os.getenv('AZURE_SEARCH_INDEX_NAME')\n",
    "search_datasource = os.environ[\"AZURE_SEARCH_DATASOURCE\"]\n",
    "search_skillset = os.environ[\"AZURE_SEARCH_SKILLSET\"]\n",
    "search_indexer = os.environ[\"AZURE_SEARCH_INDEXER\"]\n",
    "search_datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index_client = SearchIndexClient(endpoint=search_endpoint , credential=AzureKeyCredential(api_key))\n",
    "\n",
    "index = create_search_index(\n",
    "    search_index ,\n",
    "    openai.api_base,\n",
    "    azure_openai_embedding_deployment_id,\n",
    "    openai.api_key\n",
    ")\n",
    "search_index_client.create_or_update_index(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_indexer_client = SearchIndexerClient(endpoint=search_endpoint , credential=AzureKeyCredential(api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = create_search_datasource(\n",
    "    search_datasource,\n",
    "    blob_connection_string,\n",
    "    blob_container\n",
    ")\n",
    "search_indexer_client.create_or_update_data_source_connection(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillset = create_search_skillset(\n",
    "    search_skillset,\n",
    "    search_index,\n",
    "    azure_openai_endpoint,\n",
    "    azure_openai_embedding_deployment_id,\n",
    "    azure_openai_key,\n",
    "    text_split_mode='pages',\n",
    "    maximum_page_length=2000,\n",
    "    page_overlap_length=500\n",
    ")\n",
    "search_indexer_client.create_or_update_skillset(skillset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = create_search_indexer(\n",
    "    indexer_name=search_indexer,\n",
    "    index_name=search_index,\n",
    "    datasource_name=search_datasource,\n",
    "    skillset_name=search_skillset\n",
    ")\n",
    "search_indexer_client.create_or_update_indexer(indexer)\n",
    "search_indexer_client.run_indexer(search_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = search_index_client.get_search_client(search_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the documents using Azure Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "queries = df_ragas['query'].tolist()\n",
    "\n",
    "for query in queries:\n",
    "    search_results = search_client.search(\n",
    "            search_text=query,\n",
    "            search_fields=[\"chunk\"],  # Adjust to match your index schema\n",
    "            select=[\"chunk\"],  # Fields you want to retrieve\n",
    "            top=5  # Limit the number of results\n",
    "        )\n",
    "\n",
    "    # Collect relevant contexts\n",
    "    relevant_contexts = [result[\"chunk\"] for result in search_results]\n",
    "\n",
    "    combined_context = \"\\n\".join(relevant_contexts)\n",
    "\n",
    "    if not combined_context:\n",
    "        print(\"No relevant contexts found.\")\n",
    "        combined_context = texts[0]\n",
    "\n",
    "    prompt = (\n",
    "    f\"Sources:\\n{combined_context}\\n\\n\"\n",
    "    \"There is a list of queries and one source document. \"\n",
    "    \"Answer each query using only the sources provided above. \"\n",
    "    \"Structure your response for each query as follows:\\n\"\n",
    "    \"Query: [The query]\\n\"\n",
    "    \"Answer: [Your answer to the query].\\n\\n\"\n",
    "    )\n",
    "\n",
    "    prompt += f\"Query: {query}\\n\"\n",
    "\n",
    "    # Send the batch query to OpenAI\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Use your Azure OpenAI deployment name\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200  # Adjust as needed for large responses\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    answer = None\n",
    "    answer = response_content.split(\"Answer:\")[1].strip()\n",
    "\n",
    "    results.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": combined_context,\n",
    "        \"response\": answer\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['reference'] = df_ragas['reference_answer']\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    api_version=os.environ['OPENAI_API_VERSION'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']\n",
    ")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"gpt-4o\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    api_version=os.environ['OPENAI_API_VERSION'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_samples = []\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for _, row in df.iterrows():  # Use `iterrows()` to access rows in the DataFrame\n",
    "    list_of_samples.append(\n",
    "        SingleTurnSample(\n",
    "            user_input=row[\"user_input\"],  # Map user_input\n",
    "            reference=row[\"reference\"],  # Map reference\n",
    "            response=row[\"response\"],  # Map response\n",
    "            retrieved_contexts=[row[\"retrieved_contexts\"]]  # Wrap retrieved_contexts in a list\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create an EvaluationDataset\n",
    "ragas_evaluation_dataset = EvaluationDataset(list_of_samples)\n",
    "\n",
    "# Convert the EvaluationDataset to a pandas DataFrame for sampling\n",
    "ragas_evaluation_dataset.to_pandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LlamaIndexLLMWrapper(llm)\n",
    "evaluator_embeddings = LlamaIndexEmbeddingsWrapper(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm)\n",
    "]\n",
    "\n",
    "\n",
    "ragas_evaluation_result = evaluate(\n",
    "    dataset=ragas_evaluation_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    run_config=RunConfig(timeout=1800, max_wait=180, max_retries=20),\n",
    "    show_progress=True,\n",
    "    batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ragas_result = ragas_evaluation_result.to_pandas()\n",
    "\n",
    "# Removing NULL values\n",
    "df_ragas_result = df_ragas_result[(\n",
    "    ~df_ragas_result['faithfulness'].isnull()\n",
    ")&(\n",
    "    ~df_ragas_result['context_precision'].isnull()\n",
    ")&(\n",
    "    ~df_ragas_result['context_recall'].isnull()\n",
    ")].reset_index(drop=True)\n",
    "\n",
    "df_ragas_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ragas_result.to_json('./test-dataset-azure.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
